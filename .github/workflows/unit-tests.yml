name: Unit Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11', '3.12']

    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-cov pytest-xdist

    - name: Set up test environment
      run: |
        echo "DATABASE_URL=sqlite:///./test.db" >> $GITHUB_ENV
        echo "REDIS_URL=redis://localhost:6379/0" >> $GITHUB_ENV
        echo "API_TITLE=TODO API Test" >> $GITHUB_ENV
        echo "API_VERSION=1.0.0" >> $GITHUB_ENV
        echo "CAS_ROOT_PATH=./test_blobs" >> $GITHUB_ENV
        echo "FILE_STORAGE_ROOT=./test_storage" >> $GITHUB_ENV
        echo "GIT_REPO_PATH=./test_git_repo" >> $GITHUB_ENV

    - name: Create test directories
      run: |
        mkdir -p test_blobs test_storage test_git_repo

    - name: Create database tables
      run: |
        python -c "
        from app.core.database import engine, Base
        Base.metadata.create_all(bind=engine)
        print('Database tables created successfully')
        "

    - name: Run database migrations
      run: |
        alembic upgrade head || echo "Migration failed, but continuing..."

    - name: Check test files exist
      run: |
        echo "Checking for test files..."
        ls -la tests/ || echo "tests/ directory not found"
        find . -name "test_*.py" -type f | head -10 || echo "No test files found"

    - name: Run unit tests with coverage
      run: |
        if [ -d "tests" ]; then
          echo "Running tests from tests/ directory..."
          pytest tests/ -k "not integration" -v --cov=app --cov-report=xml --cov-report=html --cov-report=term --junitxml=test-results.xml
        else
          echo "tests/ directory not found, looking for test files in current directory..."
          pytest . -k "not integration" -v --cov=app --cov-report=xml --cov-report=html --cov-report=term --junitxml=test-results.xml
        fi

    - name: Run specific unit test categories
      run: |
        # Run backup service unit tests if they exist
        if [ -f "tests/test_backup_service_ut.py" ]; then
          echo "Running backup service unit tests..."
          pytest tests/test_backup_service_ut.py -v
        fi
        
        # Run review service unit tests if they exist
        if [ -f "tests/test_review_service_ut.py" ]; then
          echo "Running review service unit tests..."
          pytest tests/test_review_service_ut.py -v
        fi
        
        # Run hierarchical ID service unit tests if they exist
        if [ -f "tests/test_hierarchical_id_service_ut.py" ]; then
          echo "Running hierarchical ID service unit tests..."
          pytest tests/test_hierarchical_id_service_ut.py -v
        fi
        
        # Run CAS service unit tests if they exist
        if [ -f "tests/test_cas_service_ut.py" ]; then
          echo "Running CAS service unit tests..."
          pytest tests/test_cas_service_ut.py -v
        fi
        
        # Run config unit tests if they exist
        if [ -f "tests/test_config_ut.py" ]; then
          echo "Running config unit tests..."
          pytest tests/test_config_ut.py -v
        fi
        
        # Run enhanced task service unit tests if they exist
        if [ -f "tests/test_enhanced_task_service_ut.py" ]; then
          echo "Running enhanced task service unit tests..."
          pytest tests/test_enhanced_task_service_ut.py -v
        fi
        
        # Run TDD hook service unit tests if they exist
        if [ -f "tests/test_tdd_hook_service_ut.py" ]; then
          echo "Running TDD hook service unit tests..."
          pytest tests/test_tdd_hook_service_ut.py -v
        fi
        
        # Run git service unit tests if they exist
        if [ -f "tests/test_git_service_ut.py" ]; then
          echo "Running git service unit tests..."
          pytest tests/test_git_service_ut.py -v
        fi

    - name: Run additional unit tests
      run: |
        # Run concurrent operation tests if they exist
        if [ -f "tests/test_concurrent_operations_ut.py" ]; then
          echo "Running concurrent operation tests..."
          pytest tests/test_concurrent_operations_ut.py -v
        fi
        
        # Run error case tests if they exist
        if [ -f "tests/test_error_cases_enhanced.py" ]; then
          echo "Running error case tests..."
          pytest tests/test_error_cases_enhanced.py -v
        fi
        
        # Run status transition tests if they exist
        if [ -f "tests/test_status_transition_ut.py" ]; then
          echo "Running status transition tests..."
          pytest tests/test_status_transition_ut.py -v
        fi
        
        # Run navigation tests if they exist
        if [ -f "tests/test_hierarchical_navigation.py" ]; then
          echo "Running navigation tests..."
          pytest tests/test_hierarchical_navigation.py -v
        fi
        
        # Run search filter tests if they exist
        if [ -f "tests/test_search_filter.py" ]; then
          echo "Running search filter tests..."
          pytest tests/test_search_filter.py -v
        fi
        
        # Run comments history tests if they exist
        if [ -f "tests/test_comments_history_ut.py" ]; then
          echo "Running comments history tests..."
          pytest tests/test_comments_history_ut.py -v
        fi
        
        # Run tree API tests if they exist
        if [ -f "tests/test_tree_api.py" ]; then
          echo "Running tree API tests..."
          pytest tests/test_tree_api.py -v
        fi
        
        # Run CAS artifacts tests if they exist
        if [ -f "tests/test_cas_artifacts.py" ]; then
          echo "Running CAS artifacts tests..."
          pytest tests/test_cas_artifacts.py -v
        fi
        
        # Run storage management tests if they exist
        if [ -f "tests/test_storage_management.py" ]; then
          echo "Running storage management tests..."
          pytest tests/test_storage_management.py -v
        fi
        
        # Run updated_at management tests if they exist
        if [ -f "tests/test_updated_at_management.py" ]; then
          echo "Running updated_at management tests..."
          pytest tests/test_updated_at_management.py -v
        fi
        
        # Run review management tests if they exist
        if [ -f "tests/test_review_management.py" ]; then
          echo "Running review management tests..."
          pytest tests/test_review_management.py -v
        fi
        
        # Run hierarchical review management tests if they exist
        if [ -f "tests/test_hierarchical_review_management.py" ]; then
          echo "Running hierarchical review management tests..."
          pytest tests/test_hierarchical_review_management.py -v
        fi
        
        # Run review API unit tests if they exist
        if [ -f "tests/test_review_api_ut.py" ]; then
          echo "Running review API unit tests..."
          pytest tests/test_review_api_ut.py -v
        fi

    - name: Run all unit tests summary
      run: |
        echo "Running all available unit tests..."
        pytest tests/ -k "not integration" --collect-only -q | grep -c "test session starts" || echo "No unit tests found"

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: unit-test-results-${{ matrix.python-version }}
        path: |
          htmlcov/
          coverage.xml
          test-results.xml
          .pytest_cache/

    - name: Comment PR with test results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Read test results
          let testResults = '';
          try {
            const testResultsPath = 'test-results.xml';
            if (fs.existsSync(testResultsPath)) {
              testResults = fs.readFileSync(testResultsPath, 'utf8');
            }
          } catch (error) {
            console.log('Could not read test results:', error);
          }
          
          // Create comment
          const comment = `## ðŸ§ª Unit Test Results
          
          **Python Version**: ${{ matrix.python-version }}
          **Status**: ${{ job.status }}
          
          ### Test Summary
          - All unit tests have been executed
          - Coverage report generated
          - Test results uploaded as artifacts
          
          ### Coverage
          Coverage report is available in the artifacts section.
          
          ### Next Steps
          - Review any failing tests
          - Check coverage report for areas needing more tests
          - Update tests if needed`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
